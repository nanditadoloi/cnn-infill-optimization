{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Wrangling\n",
    "porosity_df = pd.read_csv('X-poro_15_55_5_train.csv',header=None)\n",
    "porosity_np = porosity_df.to_numpy()\n",
    "porosity_np = porosity_np[:,0:4125]\n",
    "porosity_np = np.repeat(porosity_np, repeats=500, axis=0)\n",
    "porosity_list = []\n",
    "\n",
    "\n",
    "\n",
    "porosity_validation_df = pd.read_csv('X-poro-validation.csv',header=None)\n",
    "porosity_validation_np = porosity_validation_df.to_numpy()\n",
    "porosity_validation_np = porosity_validation_np[:,0:4125]\n",
    "porosity_validation_np = np.repeat(porosity_validation_np, repeats=100, axis=0)\n",
    "porosity_validation_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(500):\n",
    "    new_data_infil = np.reshape(porosity_np[i,:],(15,55,5))\n",
    "    porosity_list.append(new_data_infil)\n",
    "    \n",
    "    \n",
    "for i in range(100):\n",
    "    new_data_infil_validation = np.reshape(porosity_validation_np[i,:],(15,5,5))\n",
    "    porosity_validation_list.append(new_data_infil_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "saturation_df = pd.read_csv('X-saturation.csv',header=None)\n",
    "saturation_np = saturation_df.to_numpy()\n",
    "saturation_np = saturation_np[:,0:125]\n",
    "saturation_list = []\n",
    "\n",
    "\n",
    "saturation_validation_df = pd.read_csv('X-saturation-validation.csv',header=None)\n",
    "saturation_validation_np = saturation_validation_df.to_numpy()\n",
    "saturation_validation_np = saturation_validation_np[:,0:125]\n",
    "saturation_validation_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(500):\n",
    "    new_data_infil = np.zeros((5,5,5))\n",
    "    new_data_infil = np.reshape(saturation_np[i,:],(5,5,5))\n",
    "    saturation_list.append(new_data_infil)\n",
    "    \n",
    "    \n",
    "for i in range(100):\n",
    "    new_data_infil_validation = np.zeros((5,5,5))\n",
    "    new_data_infil_validation = np.reshape(saturation_validation_np[i,:],(5,5,5))\n",
    "    saturation_validation_list.append(new_data_infil_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 126)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saturation_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pressure_df = pd.read_csv('X-pressure.csv',header=None)\n",
    "pressure_np = pressure_df.to_numpy()\n",
    "pressure_np = pressure_np[:,0:125]\n",
    "pressure_list = []\n",
    "\n",
    "\n",
    "pressure_validation_df = pd.read_csv('X-pressure-validation.csv',header=None)\n",
    "pressure_validation_np = pressure_validation_df.to_numpy()\n",
    "pressure_validation_np = pressure_validation_np[:,0:125]\n",
    "pressure_validation_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(500):\n",
    "    new_data_infil = np.zeros((5,5,5))\n",
    "    new_data_infil = np.reshape(pressure_np[i,:],(5,5,5))\n",
    "    pressure_list.append(new_data_infil)\n",
    "    \n",
    "    \n",
    "for i in range(100):\n",
    "    new_data_infil_validation = np.zeros((5,5,5))\n",
    "    new_data_infil_validation = np.reshape(pressure_validation_np[i,:],(5,5,5))\n",
    "    pressure_validation_list.append(new_data_infil_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "permeability_df = pd.read_csv('X-perm.csv',header=None)\n",
    "permeability_np = permeability_df.to_numpy()\n",
    "permeability_np_x = permeability_np[0:500,0:125]\n",
    "permeability_np_y = permeability_np[500:1000,0:125]\n",
    "permeability_np_z = permeability_np[1000:1500,0:125]\n",
    "permeability_np = (permeability_np_x+permeability_np_y+permeability_np_z)/3.0\n",
    "permeability_list = []\n",
    "\n",
    "\n",
    "permeability_validation_df = pd.read_csv('X-perm-validation.csv',header=None)\n",
    "permeability_validation_np = permeability_validation_df.to_numpy()\n",
    "permeability_validation_np_x = permeability_validation_np[0:100,0:125]\n",
    "permeability_validation_np_y = permeability_validation_np[100:200,0:125]\n",
    "permeability_validation_np_z = permeability_validation_np[200:300,0:125]\n",
    "permeability_validation_np = (permeability_validation_np_x+permeability_validation_np_y+permeability_validation_np_z)/3.0\n",
    "permeability_validation_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(500):\n",
    "    new_data_infil = np.zeros((5,5,5))\n",
    "    new_data_infil = np.reshape(permeability_np[i,:],(5,5,5))\n",
    "    permeability_list.append(new_data_infil)\n",
    "    \n",
    "    \n",
    "for i in range(100):\n",
    "    new_data_infil_validation = np.zeros((5,5,5))\n",
    "    new_data_infil_validation = np.reshape(permeability_validation_np[i,:],(5,5,5))\n",
    "    permeability_validation_list.append(new_data_infil_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "auxiliary_df = pd.read_csv('X-auxilary.csv',header=None)\n",
    "auxiliary_np = auxiliary_df.to_numpy()\n",
    "auxiliary_np = auxiliary_np[:,0:5]\n",
    "auxiliary_list = []\n",
    "\n",
    "auxiliary_validation_df = pd.read_csv('X-auxilary-validation.csv',header=None)\n",
    "auxiliary_validation_np = auxiliary_validation_df.to_numpy()\n",
    "auxiliary_validation_np = auxiliary_validation_np[:,0:5]\n",
    "auxiliary_validation_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(500):\n",
    "    auxiliary_list.append(auxiliary_np[i,:])\n",
    "    \n",
    "\n",
    "for i in range(100):\n",
    "    auxiliary_validation_list.append(auxiliary_validation_np[i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOPT_df = pd.read_csv('Y-FOPT.csv',header=None)\n",
    "FOPT_np = FOPT_df.to_numpy()\n",
    "FOPT_np = FOPT_np[:,0]\n",
    "FOPT_list = []\n",
    "\n",
    "FOPT_validation_df = pd.read_csv('Y-FOPT-validation.csv',header=None)\n",
    "FOPT_validation_np = FOPT_validation_df.to_numpy()\n",
    "FOPT_validation_np = FOPT_validation_np[:,0]\n",
    "FOPT_validation_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(500):\n",
    "    FOPT_list.append([FOPT_np[i]])\n",
    "    \n",
    "for i in range(100):\n",
    "    FOPT_validation_list.append(FOPT_validation_np[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### (1): Data\n",
    "training_X_perm = np.array(permeability_list)\n",
    "training_X_poro = np.array(porosity_list)\n",
    "training_X_satu = np.array(saturation_list)\n",
    "training_X_pres = np.array(pressure_list)\n",
    "training_X_auxu = np.array(auxiliary_list)\n",
    "training_Y = np.array(FOPT_list)/10000.0\n",
    "\n",
    "\n",
    "\n",
    "validation_X_perm = np.array(permeability_validation_list)\n",
    "validation_X_poro = np.array(porosity_validation_list)\n",
    "validation_X_satu = np.array(saturation_validation_list)\n",
    "validation_X_pres = np.array(pressure_validation_list)\n",
    "validation_X_auxu = np.array(auxiliary_validation_list)\n",
    "validation_Y = np.array(FOPT_validation_list)/10000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Format Channels for CNN Networks\n",
    "training_X_perm = np.expand_dims(training_X_perm, axis=1)\n",
    "training_X_poro = np.expand_dims(training_X_poro, axis=1)\n",
    "training_X_satu = np.expand_dims(training_X_satu, axis=1)\n",
    "training_X_pres = np.expand_dims(training_X_pres, axis=1)\n",
    "pt_training_X_perm = Variable(torch.from_numpy(training_X_perm).float(), requires_grad=False).to(device)\n",
    "pt_training_X_poro = Variable(torch.from_numpy(training_X_poro).float(), requires_grad=False).to(device)\n",
    "pt_training_X_satu = Variable(torch.from_numpy(training_X_satu).float(), requires_grad=False).to(device)\n",
    "pt_training_X_pres = Variable(torch.from_numpy(training_X_pres).float(), requires_grad=False).to(device)\n",
    "pt_training_X_auxu = Variable(torch.from_numpy(training_X_auxu).float(), requires_grad=False).to(device)\n",
    "pt_training_Y = Variable(torch.from_numpy(training_Y).float(), requires_grad=False).to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "validation_X_perm = np.expand_dims(validation_X_perm, axis=1)\n",
    "validation_X_poro = np.expand_dims(validation_X_poro, axis=1)\n",
    "validation_X_satu = np.expand_dims(validation_X_satu, axis=1)\n",
    "validation_X_pres = np.expand_dims(validation_X_pres, axis=1)\n",
    "validation_Y = validation_Y.reshape(-1,1)\n",
    "pt_validation_X_perm = Variable(torch.from_numpy(validation_X_perm).float(), requires_grad=False).to(device)\n",
    "pt_validation_X_poro = Variable(torch.from_numpy(validation_X_poro).float(), requires_grad=False).to(device)\n",
    "pt_validation_X_satu = Variable(torch.from_numpy(validation_X_satu).float(), requires_grad=False).to(device)\n",
    "pt_validation_X_pres = Variable(torch.from_numpy(validation_X_pres).float(), requires_grad=False).to(device)\n",
    "pt_validation_X_auxu = Variable(torch.from_numpy(validation_X_auxu).float(), requires_grad=False).to(device)\n",
    "pt_validation_Y = Variable(torch.from_numpy(validation_Y).float(), requires_grad=False).to(device)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# CNN Formula: W_new = ((W-F+2P)/S) +1\n",
    "# W = Width of the input\n",
    "# F = CNN filter width\n",
    "# P = Padding: Width of zeros around the data\n",
    "# S = Stride: Number of skips make by the filter\n",
    "w_new = ((5-3+2)/1) + 1 = 5\n",
    "wnew = (5-2+2p) + 1 = 4+2p : p=0: w_new = 4\n",
    "        1 X 5x5x5\n",
    "        Our First CNN Layer:\n",
    "            W=5 H=5 D=5\n",
    "            F=2   2   2\n",
    "            P=0   0   0\n",
    "            S=1   1   1\n",
    "            C=100\n",
    "            W_new=4 4 4\n",
    "#         Maxpool:\n",
    "#             W_new=2 2 2\n",
    "#             C=100\n",
    "        2nd CNN layer:\n",
    "            W=4 4 4\n",
    "            F=2\n",
    "            P=0\n",
    "            S=1\n",
    "            C=100\n",
    "            W_new=3 3 3\n",
    "        3rd CNN Layer:\n",
    "            W=3\n",
    "            F=2\n",
    "            P=0\n",
    "            S=1\n",
    "            C=100\n",
    "            W_new=2 2 2 \n",
    "            100*2*2*2 = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN100(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN100, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv3d(in_channels=1, out_channels=100, kernel_size=[2,2,2], stride=1, padding=0)\n",
    "        self.conv2 = torch.nn.Conv3d(in_channels=100, out_channels=100, kernel_size=[2,2,2], stride=1, padding=0)\n",
    "        self.conv3 = torch.nn.Conv3d(in_channels=100, out_channels=100, kernel_size=[2,2,2], stride=1, padding=0)\n",
    "        self.batchnorm = torch.nn.BatchNorm3d(num_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm_x = self.batchnorm(x)\n",
    "        conv1_out = torch.nn.functional.relu(self.conv1(norm_x))\n",
    "        conv2_out = torch.nn.functional.relu(self.conv2(conv1_out))\n",
    "        conv3_out = torch.nn.functional.relu(self.conv3(conv2_out))\n",
    "        flat_out = conv3_out.reshape(conv3_out.shape[0],-1)\n",
    "        return flat_out\n",
    "\n",
    "class CNN20(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN20, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv3d(in_channels=1, out_channels=20, kernel_size=[2,2,2], stride=1, padding=0)\n",
    "        self.conv2 = torch.nn.Conv3d(in_channels=20, out_channels=20, kernel_size=[2,2,2], stride=1, padding=0)\n",
    "        self.conv3 = torch.nn.Conv3d(in_channels=20, out_channels=20, kernel_size=[2,2,2], stride=1, padding=0)\n",
    "        self.batchnorm = torch.nn.BatchNorm3d(num_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm_x = self.batchnorm(x)\n",
    "        conv1_out = torch.nn.functional.relu(self.conv1(norm_x))\n",
    "        conv2_out = torch.nn.functional.relu(self.conv2(conv1_out))\n",
    "        conv3_out = torch.nn.functional.relu(self.conv3(conv2_out))\n",
    "        flat_out = conv3_out.reshape(conv3_out.shape[0],-1)\n",
    "        return flat_out\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.perm_cnn = CNN100()\n",
    "        self.poro_cnn = CNN100()\n",
    "        self.satu_cnn = CNN20()\n",
    "        self.pres_cnn = CNN20()\n",
    "        self.batchnorm = torch.nn.BatchNorm1d(num_features=1925)\n",
    "        \n",
    "        self.lin1 = torch.nn.Linear(in_features=1925, out_features=500)\n",
    "        self.lin2 = torch.nn.Linear(in_features=500, out_features=100)\n",
    "        self.lin3 = torch.nn.Linear(in_features=100, out_features=30)\n",
    "        self.ouput = torch.nn.Linear(in_features=30, out_features=1)\n",
    "    \n",
    "    def forward(self, Xperm, Xporo, Xsatu, Xpres, Xauxu):\n",
    "        poro_out = self.poro_cnn(Xporo)\n",
    "        perm_out = self.perm_cnn(Xperm)\n",
    "        satu_out = self.satu_cnn(Xsatu)\n",
    "        pres_out = self.pres_cnn(Xpres)\n",
    "        #Concatenation\n",
    "        con_out = torch.cat([perm_out,poro_out,satu_out,pres_out,Xauxu],1)\n",
    "        con_out_norm = self.batchnorm(con_out)\n",
    "        lin1_out = torch.nn.functional.relu(self.lin1(con_out_norm))\n",
    "        lin2_out = torch.nn.functional.relu(self.lin2(lin1_out))\n",
    "        lin3_out = torch.nn.functional.relu(self.lin3(lin2_out))\n",
    "        fopt = self.ouput(lin3_out)  \n",
    "        return fopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### (2) Model\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = Net()\n",
    "net = net.to(device)\n",
    "cost_function = torch.nn.MSELoss() # Mean squared error\n",
    "#optimizer = torch.optim.SGD(net.parameters(), lr=0.1) # SGD: Stochastic Gradient Descent\n",
    "optimizer = torch.optim.Adam(net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Traning Loss: tensor(4.1875) Validation Loss: tensor(3.6149)\n",
      "1 Traning Loss: tensor(3.7944) Validation Loss: tensor(3.2014)\n",
      "2 Traning Loss: tensor(3.3668) Validation Loss: tensor(2.6033)\n",
      "3 Traning Loss: tensor(2.7469) Validation Loss: tensor(1.7879)\n",
      "4 Traning Loss: tensor(1.9105) Validation Loss: tensor(0.9745)\n",
      "5 Traning Loss: tensor(1.0457) Validation Loss: tensor(0.6397)\n",
      "6 Traning Loss: tensor(0.5914) Validation Loss: tensor(0.8529)\n",
      "7 Traning Loss: tensor(0.7132) Validation Loss: tensor(0.6971)\n",
      "8 Traning Loss: tensor(0.5754) Validation Loss: tensor(0.3660)\n",
      "9 Traning Loss: tensor(0.2703) Validation Loss: tensor(0.2313)\n",
      "10 Traning Loss: tensor(0.1442) Validation Loss: tensor(0.2773)\n",
      "11 Traning Loss: tensor(0.1848) Validation Loss: tensor(0.3113)\n",
      "12 Traning Loss: tensor(0.2204) Validation Loss: tensor(0.2685)\n",
      "13 Traning Loss: tensor(0.1839) Validation Loss: tensor(0.1925)\n",
      "14 Traning Loss: tensor(0.1181) Validation Loss: tensor(0.1409)\n",
      "15 Traning Loss: tensor(0.0822) Validation Loss: tensor(0.1312)\n",
      "16 Traning Loss: tensor(0.0841) Validation Loss: tensor(0.1282)\n",
      "17 Traning Loss: tensor(0.0795) Validation Loss: tensor(0.1164)\n",
      "18 Traning Loss: tensor(0.0557) Validation Loss: tensor(0.1145)\n",
      "19 Traning Loss: tensor(0.0420) Validation Loss: tensor(0.1246)\n",
      "20 Traning Loss: tensor(0.0471) Validation Loss: tensor(0.1292)\n",
      "21 Traning Loss: tensor(0.0520) Validation Loss: tensor(0.1158)\n",
      "22 Traning Loss: tensor(0.0441) Validation Loss: tensor(0.0930)\n",
      "23 Traning Loss: tensor(0.0293) Validation Loss: tensor(0.0761)\n",
      "24 Traning Loss: tensor(0.0205) Validation Loss: tensor(0.0723)\n",
      "25 Traning Loss: tensor(0.0224) Validation Loss: tensor(0.0738)\n",
      "26 Traning Loss: tensor(0.0261) Validation Loss: tensor(0.0711)\n",
      "27 Traning Loss: tensor(0.0231) Validation Loss: tensor(0.0657)\n",
      "28 Traning Loss: tensor(0.0166) Validation Loss: tensor(0.0634)\n",
      "29 Traning Loss: tensor(0.0147) Validation Loss: tensor(0.0654)\n",
      "30 Traning Loss: tensor(0.0173) Validation Loss: tensor(0.0671)\n",
      "31 Traning Loss: tensor(0.0182) Validation Loss: tensor(0.0646)\n",
      "32 Traning Loss: tensor(0.0148) Validation Loss: tensor(0.0601)\n",
      "33 Traning Loss: tensor(0.0107) Validation Loss: tensor(0.0578)\n",
      "34 Traning Loss: tensor(0.0099) Validation Loss: tensor(0.0564)\n",
      "35 Traning Loss: tensor(0.0114) Validation Loss: tensor(0.0545)\n",
      "36 Traning Loss: tensor(0.0114) Validation Loss: tensor(0.0526)\n",
      "37 Traning Loss: tensor(0.0089) Validation Loss: tensor(0.0535)\n",
      "38 Traning Loss: tensor(0.0069) Validation Loss: tensor(0.0572)\n",
      "39 Traning Loss: tensor(0.0072) Validation Loss: tensor(0.0604)\n",
      "40 Traning Loss: tensor(0.0083) Validation Loss: tensor(0.0596)\n",
      "41 Traning Loss: tensor(0.0078) Validation Loss: tensor(0.0550)\n",
      "42 Traning Loss: tensor(0.0059) Validation Loss: tensor(0.0499)\n",
      "43 Traning Loss: tensor(0.0050) Validation Loss: tensor(0.0468)\n",
      "44 Traning Loss: tensor(0.0056) Validation Loss: tensor(0.0456)\n",
      "45 Traning Loss: tensor(0.0059) Validation Loss: tensor(0.0451)\n",
      "46 Traning Loss: tensor(0.0051) Validation Loss: tensor(0.0453)\n",
      "47 Traning Loss: tensor(0.0042) Validation Loss: tensor(0.0460)\n",
      "48 Traning Loss: tensor(0.0042) Validation Loss: tensor(0.0461)\n",
      "49 Traning Loss: tensor(0.0045) Validation Loss: tensor(0.0452)\n",
      "50 Traning Loss: tensor(0.0042) Validation Loss: tensor(0.0438)\n",
      "51 Traning Loss: tensor(0.0034) Validation Loss: tensor(0.0432)\n",
      "52 Traning Loss: tensor(0.0030) Validation Loss: tensor(0.0436)\n",
      "53 Traning Loss: tensor(0.0032) Validation Loss: tensor(0.0441)\n",
      "54 Traning Loss: tensor(0.0034) Validation Loss: tensor(0.0440)\n",
      "55 Traning Loss: tensor(0.0030) Validation Loss: tensor(0.0438)\n",
      "56 Traning Loss: tensor(0.0026) Validation Loss: tensor(0.0442)\n",
      "57 Traning Loss: tensor(0.0026) Validation Loss: tensor(0.0446)\n",
      "58 Traning Loss: tensor(0.0027) Validation Loss: tensor(0.0445)\n",
      "59 Traning Loss: tensor(0.0024) Validation Loss: tensor(0.0438)\n",
      "60 Traning Loss: tensor(0.0021) Validation Loss: tensor(0.0429)\n",
      "61 Traning Loss: tensor(0.0020) Validation Loss: tensor(0.0420)\n",
      "62 Traning Loss: tensor(0.0022) Validation Loss: tensor(0.0415)\n",
      "63 Traning Loss: tensor(0.0021) Validation Loss: tensor(0.0417)\n",
      "64 Traning Loss: tensor(0.0018) Validation Loss: tensor(0.0425)\n",
      "65 Traning Loss: tensor(0.0017) Validation Loss: tensor(0.0432)\n",
      "66 Traning Loss: tensor(0.0017) Validation Loss: tensor(0.0431)\n",
      "67 Traning Loss: tensor(0.0017) Validation Loss: tensor(0.0423)\n",
      "68 Traning Loss: tensor(0.0015) Validation Loss: tensor(0.0415)\n",
      "69 Traning Loss: tensor(0.0014) Validation Loss: tensor(0.0411)\n",
      "70 Traning Loss: tensor(0.0015) Validation Loss: tensor(0.0411)\n",
      "71 Traning Loss: tensor(0.0014) Validation Loss: tensor(0.0412)\n",
      "72 Traning Loss: tensor(0.0013) Validation Loss: tensor(0.0411)\n",
      "73 Traning Loss: tensor(0.0013) Validation Loss: tensor(0.0410)\n",
      "74 Traning Loss: tensor(0.0013) Validation Loss: tensor(0.0408)\n",
      "75 Traning Loss: tensor(0.0012) Validation Loss: tensor(0.0407)\n",
      "76 Traning Loss: tensor(0.0011) Validation Loss: tensor(0.0407)\n",
      "77 Traning Loss: tensor(0.0011) Validation Loss: tensor(0.0407)\n",
      "78 Traning Loss: tensor(0.0011) Validation Loss: tensor(0.0406)\n",
      "79 Traning Loss: tensor(0.0010) Validation Loss: tensor(0.0405)\n",
      "80 Traning Loss: tensor(0.0010) Validation Loss: tensor(0.0407)\n",
      "81 Traning Loss: tensor(0.0009) Validation Loss: tensor(0.0409)\n",
      "82 Traning Loss: tensor(0.0009) Validation Loss: tensor(0.0409)\n",
      "83 Traning Loss: tensor(0.0009) Validation Loss: tensor(0.0406)\n",
      "84 Traning Loss: tensor(0.0008) Validation Loss: tensor(0.0401)\n",
      "85 Traning Loss: tensor(0.0008) Validation Loss: tensor(0.0397)\n",
      "86 Traning Loss: tensor(0.0008) Validation Loss: tensor(0.0396)\n",
      "87 Traning Loss: tensor(0.0008) Validation Loss: tensor(0.0398)\n",
      "88 Traning Loss: tensor(0.0007) Validation Loss: tensor(0.0400)\n",
      "89 Traning Loss: tensor(0.0007) Validation Loss: tensor(0.0400)\n",
      "90 Traning Loss: tensor(0.0007) Validation Loss: tensor(0.0400)\n",
      "91 Traning Loss: tensor(0.0007) Validation Loss: tensor(0.0399)\n",
      "92 Traning Loss: tensor(0.0006) Validation Loss: tensor(0.0399)\n",
      "93 Traning Loss: tensor(0.0006) Validation Loss: tensor(0.0399)\n",
      "94 Traning Loss: tensor(0.0006) Validation Loss: tensor(0.0399)\n",
      "95 Traning Loss: tensor(0.0006) Validation Loss: tensor(0.0399)\n",
      "96 Traning Loss: tensor(0.0006) Validation Loss: tensor(0.0399)\n",
      "97 Traning Loss: tensor(0.0006) Validation Loss: tensor(0.0399)\n",
      "98 Traning Loss: tensor(0.0005) Validation Loss: tensor(0.0397)\n",
      "99 Traning Loss: tensor(0.0005) Validation Loss: tensor(0.0395)\n",
      "100 Traning Loss: tensor(0.0005) Validation Loss: tensor(0.0393)\n",
      "101 Traning Loss: tensor(0.0005) Validation Loss: tensor(0.0392)\n",
      "102 Traning Loss: tensor(0.0005) Validation Loss: tensor(0.0393)\n",
      "103 Traning Loss: tensor(0.0005) Validation Loss: tensor(0.0394)\n",
      "104 Traning Loss: tensor(0.0004) Validation Loss: tensor(0.0394)\n",
      "105 Traning Loss: tensor(0.0004) Validation Loss: tensor(0.0394)\n",
      "106 Traning Loss: tensor(0.0004) Validation Loss: tensor(0.0394)\n",
      "107 Traning Loss: tensor(0.0004) Validation Loss: tensor(0.0393)\n",
      "108 Traning Loss: tensor(0.0004) Validation Loss: tensor(0.0393)\n",
      "109 Traning Loss: tensor(0.0004) Validation Loss: tensor(0.0394)\n",
      "110 Traning Loss: tensor(0.0004) Validation Loss: tensor(0.0394)\n",
      "111 Traning Loss: tensor(0.0004) Validation Loss: tensor(0.0393)\n",
      "112 Traning Loss: tensor(0.0003) Validation Loss: tensor(0.0393)\n",
      "113 Traning Loss: tensor(0.0003) Validation Loss: tensor(0.0392)\n",
      "114 Traning Loss: tensor(0.0003) Validation Loss: tensor(0.0391)\n",
      "115 Traning Loss: tensor(0.0003) Validation Loss: tensor(0.0391)\n",
      "116 Traning Loss: tensor(0.0003) Validation Loss: tensor(0.0391)\n",
      "117 Traning Loss: tensor(0.0003) Validation Loss: tensor(0.0391)\n",
      "118 Traning Loss: tensor(0.0003) Validation Loss: tensor(0.0392)\n",
      "119 Traning Loss: tensor(0.0003) Validation Loss: tensor(0.0392)\n",
      "120 Traning Loss: tensor(0.0003) Validation Loss: tensor(0.0392)\n",
      "121 Traning Loss: tensor(0.0003) Validation Loss: tensor(0.0392)\n",
      "122 Traning Loss: tensor(0.0003) Validation Loss: tensor(0.0392)\n",
      "123 Traning Loss: tensor(0.0003) Validation Loss: tensor(0.0392)\n",
      "124 Traning Loss: tensor(0.0002) Validation Loss: tensor(0.0392)\n",
      "125 Traning Loss: tensor(0.0002) Validation Loss: tensor(0.0392)\n",
      "126 Traning Loss: tensor(0.0002) Validation Loss: tensor(0.0391)\n",
      "127 Traning Loss: tensor(0.0002) Validation Loss: tensor(0.0391)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 Traning Loss: tensor(0.0002) Validation Loss: tensor(0.0391)\n",
      "129 Traning Loss: tensor(0.0002) Validation Loss: tensor(0.0392)\n",
      "130 Traning Loss: tensor(0.0002) Validation Loss: tensor(0.0392)\n",
      "131 Traning Loss: tensor(0.0002) Validation Loss: tensor(0.0391)\n",
      "132 Traning Loss: tensor(0.0002) Validation Loss: tensor(0.0392)\n",
      "133 Traning Loss: tensor(0.0002) Validation Loss: tensor(0.0392)\n",
      "134 Traning Loss: tensor(0.0002) Validation Loss: tensor(0.0391)\n",
      "135 Traning Loss: tensor(0.0002) Validation Loss: tensor(0.0391)\n",
      "136 Traning Loss: tensor(0.0002) Validation Loss: tensor(0.0391)\n",
      "137 Traning Loss: tensor(0.0002) Validation Loss: tensor(0.0391)\n",
      "138 Traning Loss: tensor(0.0002) Validation Loss: tensor(0.0391)\n",
      "139 Traning Loss: tensor(0.0002) Validation Loss: tensor(0.0391)\n",
      "140 Traning Loss: tensor(0.0002) Validation Loss: tensor(0.0391)\n",
      "141 Traning Loss: tensor(0.0002) Validation Loss: tensor(0.0391)\n",
      "142 Traning Loss: tensor(0.0002) Validation Loss: tensor(0.0391)\n",
      "143 Traning Loss: tensor(0.0002) Validation Loss: tensor(0.0391)\n",
      "144 Traning Loss: tensor(0.0002) Validation Loss: tensor(0.0391)\n",
      "145 Traning Loss: tensor(0.0001) Validation Loss: tensor(0.0391)\n",
      "146 Traning Loss: tensor(0.0001) Validation Loss: tensor(0.0391)\n",
      "147 Traning Loss: tensor(0.0001) Validation Loss: tensor(0.0390)\n",
      "148 Traning Loss: tensor(0.0001) Validation Loss: tensor(0.0390)\n",
      "149 Traning Loss: tensor(0.0001) Validation Loss: tensor(0.0390)\n",
      "150 Traning Loss: tensor(0.0001) Validation Loss: tensor(0.0390)\n",
      "151 Traning Loss: tensor(0.0001) Validation Loss: tensor(0.0390)\n",
      "152 Traning Loss: tensor(0.0001) Validation Loss: tensor(0.0390)\n",
      "153 Traning Loss: tensor(0.0001) Validation Loss: tensor(0.0390)\n",
      "154 Traning Loss: tensor(0.0001) Validation Loss: tensor(0.0390)\n",
      "155 Traning Loss: tensor(0.0001) Validation Loss: tensor(0.0390)\n",
      "156 Traning Loss: tensor(0.0001) Validation Loss: tensor(0.0390)\n",
      "157 Traning Loss: tensor(0.0001) Validation Loss: tensor(0.0390)\n",
      "158 Traning Loss: tensor(0.0001) Validation Loss: tensor(0.0390)\n",
      "159 Traning Loss: tensor(0.0001) Validation Loss: tensor(0.0390)\n",
      "160 Traning Loss: tensor(0.0001) Validation Loss: tensor(0.0390)\n",
      "161 Traning Loss: tensor(0.0001) Validation Loss: tensor(0.0390)\n",
      "162 Traning Loss: tensor(0.0001) Validation Loss: tensor(0.0390)\n",
      "163 Traning Loss: tensor(9.7953e-05) Validation Loss: tensor(0.0390)\n",
      "164 Traning Loss: tensor(9.5820e-05) Validation Loss: tensor(0.0390)\n",
      "165 Traning Loss: tensor(9.3588e-05) Validation Loss: tensor(0.0389)\n",
      "166 Traning Loss: tensor(9.1526e-05) Validation Loss: tensor(0.0390)\n",
      "167 Traning Loss: tensor(8.9428e-05) Validation Loss: tensor(0.0390)\n",
      "168 Traning Loss: tensor(8.7321e-05) Validation Loss: tensor(0.0389)\n",
      "169 Traning Loss: tensor(8.5336e-05) Validation Loss: tensor(0.0390)\n",
      "170 Traning Loss: tensor(8.3419e-05) Validation Loss: tensor(0.0390)\n",
      "171 Traning Loss: tensor(8.1514e-05) Validation Loss: tensor(0.0389)\n",
      "172 Traning Loss: tensor(7.9689e-05) Validation Loss: tensor(0.0390)\n",
      "173 Traning Loss: tensor(7.7840e-05) Validation Loss: tensor(0.0390)\n",
      "174 Traning Loss: tensor(7.6081e-05) Validation Loss: tensor(0.0389)\n",
      "175 Traning Loss: tensor(7.4372e-05) Validation Loss: tensor(0.0389)\n",
      "176 Traning Loss: tensor(7.2613e-05) Validation Loss: tensor(0.0389)\n",
      "177 Traning Loss: tensor(7.1002e-05) Validation Loss: tensor(0.0389)\n",
      "178 Traning Loss: tensor(6.9298e-05) Validation Loss: tensor(0.0389)\n",
      "179 Traning Loss: tensor(6.7664e-05) Validation Loss: tensor(0.0389)\n",
      "180 Traning Loss: tensor(6.6148e-05) Validation Loss: tensor(0.0389)\n",
      "181 Traning Loss: tensor(6.4629e-05) Validation Loss: tensor(0.0389)\n",
      "182 Traning Loss: tensor(6.3088e-05) Validation Loss: tensor(0.0389)\n",
      "183 Traning Loss: tensor(6.1622e-05) Validation Loss: tensor(0.0389)\n",
      "184 Traning Loss: tensor(6.0236e-05) Validation Loss: tensor(0.0389)\n",
      "185 Traning Loss: tensor(5.8741e-05) Validation Loss: tensor(0.0389)\n",
      "186 Traning Loss: tensor(5.7235e-05) Validation Loss: tensor(0.0389)\n",
      "187 Traning Loss: tensor(5.5499e-05) Validation Loss: tensor(0.0388)\n",
      "188 Traning Loss: tensor(5.3821e-05) Validation Loss: tensor(0.0388)\n",
      "189 Traning Loss: tensor(5.2434e-05) Validation Loss: tensor(0.0388)\n",
      "190 Traning Loss: tensor(5.1218e-05) Validation Loss: tensor(0.0389)\n",
      "191 Traning Loss: tensor(4.9928e-05) Validation Loss: tensor(0.0388)\n",
      "192 Traning Loss: tensor(4.8651e-05) Validation Loss: tensor(0.0389)\n",
      "193 Traning Loss: tensor(4.7461e-05) Validation Loss: tensor(0.0388)\n",
      "194 Traning Loss: tensor(4.6349e-05) Validation Loss: tensor(0.0389)\n",
      "195 Traning Loss: tensor(4.5373e-05) Validation Loss: tensor(0.0387)\n",
      "196 Traning Loss: tensor(4.4825e-05) Validation Loss: tensor(0.0390)\n",
      "197 Traning Loss: tensor(4.4185e-05) Validation Loss: tensor(0.0387)\n",
      "198 Traning Loss: tensor(4.3865e-05) Validation Loss: tensor(0.0390)\n",
      "199 Traning Loss: tensor(4.2565e-05) Validation Loss: tensor(0.0387)\n"
     ]
    }
   ],
   "source": [
    "### (3) Training / Fitting\n",
    "iterations = 200\n",
    "previous_validation_loss = 99999999.0\n",
    "for epoch in range(iterations):\n",
    "    optimizer.zero_grad() # to make the gradients zero\n",
    "    net_out = net(pt_training_X_perm,pt_training_X_poro,pt_training_X_satu,pt_training_X_pres,pt_training_X_auxu) # output of h(x)\n",
    "    loss = cost_function(net_out, pt_training_Y) # average[(net_out-pt_training_Y)^2]\n",
    "    loss.backward() # This is for computing gradients using backward propagation\n",
    "    optimizer.step() # This is equivalent to : theta_new = theta_old - alpha * derivative of J w.r.t theta\n",
    "    \n",
    "    net_out = net(pt_validation_X_perm,pt_validation_X_poro,pt_validation_X_satu,pt_validation_X_pres,pt_validation_X_auxu)\n",
    "    validation_loss = cost_function(net_out, pt_validation_Y)\n",
    "    current_validation_loss = validation_loss.data.cpu()\n",
    "\n",
    "    with torch.autograd.no_grad():\n",
    "    \tprint(epoch,\"Traning Loss:\",loss.data,\"Validation Loss:\", validation_loss.data)\n",
    "    if(current_validation_loss<=previous_validation_loss):\n",
    "        torch.save(net.state_dict(), \"my_model.pt\")\n",
    "        previous_validation_loss=current_validation_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "torch.save(net.state_dict(), \"my_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.50505 ]\n",
      " [1.95891 ]\n",
      " [2.45743 ]\n",
      " [1.70782 ]\n",
      " [1.79516 ]\n",
      " [1.99541 ]\n",
      " [2.46599 ]\n",
      " [1.6336  ]\n",
      " [1.80795 ]\n",
      " [1.93642 ]\n",
      " [1.20752 ]\n",
      " [1.39932 ]\n",
      " [1.7499  ]\n",
      " [2.49171 ]\n",
      " [1.78713 ]\n",
      " [1.00055 ]\n",
      " [1.85226 ]\n",
      " [1.97785 ]\n",
      " [1.70421 ]\n",
      " [1.53527 ]\n",
      " [1.3815  ]\n",
      " [1.97205 ]\n",
      " [1.81104 ]\n",
      " [1.20752 ]\n",
      " [1.87214 ]\n",
      " [1.27831 ]\n",
      " [1.88522 ]\n",
      " [1.93629 ]\n",
      " [1.65108 ]\n",
      " [1.9785  ]\n",
      " [2.40092 ]\n",
      " [2.40092 ]\n",
      " [1.88522 ]\n",
      " [1.30231 ]\n",
      " [2.22604 ]\n",
      " [1.85344 ]\n",
      " [1.20262 ]\n",
      " [1.79457 ]\n",
      " [0.93857 ]\n",
      " [1.73739 ]\n",
      " [2.54628 ]\n",
      " [2.43059 ]\n",
      " [2.31692 ]\n",
      " [2.14395 ]\n",
      " [1.61186 ]\n",
      " [1.83784 ]\n",
      " [2.55495 ]\n",
      " [1.25418 ]\n",
      " [1.27831 ]\n",
      " [2.37819 ]\n",
      " [1.61483 ]\n",
      " [1.62835 ]\n",
      " [1.39932 ]\n",
      " [1.86064 ]\n",
      " [2.48928 ]\n",
      " [2.21572 ]\n",
      " [2.07159 ]\n",
      " [2.34603 ]\n",
      " [1.2924  ]\n",
      " [1.86299 ]\n",
      " [2.12989 ]\n",
      " [1.64073 ]\n",
      " [1.69134 ]\n",
      " [1.82798 ]\n",
      " [2.48928 ]\n",
      " [2.15827 ]\n",
      " [1.83958 ]\n",
      " [1.94612 ]\n",
      " [1.80795 ]\n",
      " [2.2397  ]\n",
      " [2.1096  ]\n",
      " [1.80203 ]\n",
      " [1.76015 ]\n",
      " [2.31855 ]\n",
      " [1.81104 ]\n",
      " [2.04012 ]\n",
      " [1.83538 ]\n",
      " [2.12448 ]\n",
      " [1.12949 ]\n",
      " [1.76534 ]\n",
      " [1.99541 ]\n",
      " [1.57401 ]\n",
      " [2.14395 ]\n",
      " [2.2397  ]\n",
      " [1.88906 ]\n",
      " [1.66779 ]\n",
      " [1.76015 ]\n",
      " [1.69195 ]\n",
      " [2.03938 ]\n",
      " [1.44203 ]\n",
      " [1.85419 ]\n",
      " [1.93629 ]\n",
      " [1.93599 ]\n",
      " [0.918068]\n",
      " [1.93721 ]\n",
      " [1.90545 ]\n",
      " [1.76015 ]\n",
      " [2.30356 ]\n",
      " [1.69301 ]\n",
      " [1.37202 ]\n",
      " [2.10038 ]\n",
      " [2.04389 ]\n",
      " [1.4534  ]\n",
      " [1.94034 ]\n",
      " [1.51954 ]\n",
      " [1.09739 ]\n",
      " [1.97349 ]\n",
      " [2.5266  ]\n",
      " [1.71407 ]\n",
      " [1.7482  ]\n",
      " [1.45235 ]\n",
      " [1.17199 ]\n",
      " [2.07815 ]\n",
      " [1.65766 ]\n",
      " [1.82499 ]\n",
      " [2.19119 ]\n",
      " [1.79833 ]\n",
      " [1.33336 ]\n",
      " [2.38387 ]\n",
      " [1.38557 ]\n",
      " [1.50958 ]\n",
      " [2.48928 ]\n",
      " [1.75052 ]\n",
      " [1.82156 ]\n",
      " [1.59287 ]\n",
      " [1.39932 ]\n",
      " [0.977326]\n",
      " [2.12989 ]\n",
      " [1.31307 ]\n",
      " [2.27401 ]\n",
      " [2.15973 ]\n",
      " [1.76639 ]\n",
      " [2.14395 ]\n",
      " [1.19638 ]\n",
      " [1.80203 ]\n",
      " [2.16008 ]\n",
      " [2.55248 ]\n",
      " [1.76826 ]\n",
      " [2.31692 ]\n",
      " [1.69489 ]\n",
      " [1.23507 ]\n",
      " [1.65645 ]\n",
      " [1.69301 ]\n",
      " [2.08272 ]\n",
      " [2.19853 ]\n",
      " [2.03512 ]\n",
      " [1.73849 ]\n",
      " [1.97785 ]\n",
      " [1.66781 ]\n",
      " [1.69589 ]\n",
      " [2.08295 ]\n",
      " [2.16848 ]\n",
      " [1.73849 ]\n",
      " [1.33036 ]\n",
      " [1.83339 ]\n",
      " [1.53527 ]\n",
      " [1.92822 ]\n",
      " [2.07159 ]\n",
      " [1.55282 ]\n",
      " [1.96449 ]\n",
      " [2.03139 ]\n",
      " [1.93642 ]\n",
      " [1.84636 ]\n",
      " [1.70421 ]\n",
      " [2.46945 ]\n",
      " [1.29061 ]\n",
      " [1.83784 ]\n",
      " [1.19638 ]\n",
      " [2.04207 ]\n",
      " [1.83339 ]\n",
      " [1.24741 ]\n",
      " [2.18136 ]\n",
      " [1.77633 ]\n",
      " [1.86064 ]\n",
      " [2.43703 ]\n",
      " [1.57694 ]\n",
      " [1.83958 ]\n",
      " [2.30168 ]\n",
      " [2.31855 ]\n",
      " [1.96025 ]\n",
      " [1.97349 ]\n",
      " [2.05769 ]\n",
      " [1.71407 ]\n",
      " [1.19773 ]\n",
      " [1.54633 ]\n",
      " [2.08272 ]\n",
      " [2.12572 ]\n",
      " [1.87992 ]\n",
      " [2.22925 ]\n",
      " [1.83058 ]\n",
      " [1.17776 ]\n",
      " [2.31182 ]\n",
      " [2.12397 ]\n",
      " [1.77853 ]\n",
      " [1.50958 ]\n",
      " [2.2508  ]\n",
      " [2.20589 ]\n",
      " [2.49171 ]\n",
      " [0.750477]\n",
      " [1.95431 ]\n",
      " [1.14984 ]\n",
      " [1.37202 ]\n",
      " [1.29474 ]\n",
      " [1.71471 ]\n",
      " [1.80192 ]\n",
      " [2.12572 ]\n",
      " [1.74778 ]\n",
      " [1.93721 ]\n",
      " [1.14984 ]\n",
      " [2.13155 ]\n",
      " [2.29061 ]\n",
      " [0.790535]\n",
      " [2.16008 ]\n",
      " [1.39932 ]\n",
      " [1.29528 ]\n",
      " [2.12448 ]\n",
      " [2.31692 ]\n",
      " [2.08105 ]\n",
      " [1.72862 ]\n",
      " [1.58631 ]\n",
      " [2.26413 ]\n",
      " [1.93306 ]\n",
      " [2.04907 ]\n",
      " [1.81104 ]\n",
      " [2.10649 ]\n",
      " [1.78713 ]\n",
      " [1.93599 ]\n",
      " [2.24637 ]\n",
      " [1.94409 ]\n",
      " [2.46437 ]\n",
      " [1.47931 ]\n",
      " [0.792423]\n",
      " [2.19799 ]\n",
      " [1.17559 ]\n",
      " [2.11329 ]\n",
      " [2.06543 ]\n",
      " [1.77643 ]\n",
      " [2.0218  ]\n",
      " [1.86299 ]\n",
      " [1.95389 ]\n",
      " [1.23507 ]\n",
      " [1.463   ]\n",
      " [1.95891 ]\n",
      " [1.8556  ]\n",
      " [1.19638 ]\n",
      " [1.98968 ]\n",
      " [2.26193 ]\n",
      " [1.71407 ]\n",
      " [1.74346 ]\n",
      " [1.83958 ]\n",
      " [1.33036 ]\n",
      " [1.21299 ]\n",
      " [2.18739 ]\n",
      " [0.636326]\n",
      " [1.82798 ]\n",
      " [2.18549 ]\n",
      " [1.19638 ]\n",
      " [1.83958 ]\n",
      " [0.859846]\n",
      " [0.970393]\n",
      " [1.79516 ]\n",
      " [2.11187 ]\n",
      " [2.34442 ]\n",
      " [2.35289 ]\n",
      " [1.69195 ]\n",
      " [2.21572 ]\n",
      " [1.41652 ]\n",
      " [1.64759 ]\n",
      " [1.91921 ]\n",
      " [2.2194  ]\n",
      " [2.12599 ]\n",
      " [1.43158 ]\n",
      " [1.42189 ]\n",
      " [1.99105 ]\n",
      " [2.08272 ]\n",
      " [2.27401 ]\n",
      " [1.40161 ]\n",
      " [2.02782 ]\n",
      " [1.62835 ]\n",
      " [1.58631 ]\n",
      " [1.88906 ]\n",
      " [1.87684 ]\n",
      " [1.69489 ]\n",
      " [2.09331 ]\n",
      " [0.93857 ]\n",
      " [2.24637 ]\n",
      " [1.7808  ]\n",
      " [1.73739 ]\n",
      " [1.94612 ]\n",
      " [1.7709  ]\n",
      " [2.2397  ]\n",
      " [1.42189 ]\n",
      " [1.87684 ]\n",
      " [1.59058 ]\n",
      " [1.67941 ]\n",
      " [2.24244 ]\n",
      " [1.9316  ]\n",
      " [1.18707 ]\n",
      " [1.30231 ]\n",
      " [1.96449 ]\n",
      " [1.85353 ]\n",
      " [2.22513 ]\n",
      " [2.13457 ]\n",
      " [1.65108 ]\n",
      " [1.29528 ]\n",
      " [1.12949 ]\n",
      " [2.02609 ]\n",
      " [1.56242 ]\n",
      " [2.12989 ]\n",
      " [2.22554 ]\n",
      " [2.05769 ]\n",
      " [1.66634 ]\n",
      " [1.90728 ]\n",
      " [1.81707 ]\n",
      " [2.03938 ]\n",
      " [1.33961 ]\n",
      " [1.59287 ]\n",
      " [2.0218  ]\n",
      " [1.87684 ]\n",
      " [1.70782 ]\n",
      " [2.17698 ]\n",
      " [1.12025 ]\n",
      " [1.88948 ]\n",
      " [2.32727 ]\n",
      " [1.83489 ]\n",
      " [1.78049 ]\n",
      " [1.36858 ]\n",
      " [1.57401 ]\n",
      " [2.5266  ]\n",
      " [1.18707 ]\n",
      " [1.95798 ]\n",
      " [0.977326]\n",
      " [1.77714 ]\n",
      " [2.38677 ]\n",
      " [2.34722 ]\n",
      " [1.76667 ]\n",
      " [1.56523 ]\n",
      " [1.25702 ]\n",
      " [1.71959 ]\n",
      " [1.9414  ]\n",
      " [2.48928 ]\n",
      " [1.9612  ]\n",
      " [1.20752 ]\n",
      " [2.34603 ]\n",
      " [2.38387 ]\n",
      " [1.55991 ]\n",
      " [1.5618  ]\n",
      " [2.25951 ]\n",
      " [1.51518 ]\n",
      " [2.12448 ]\n",
      " [1.42189 ]\n",
      " [2.13303 ]\n",
      " [1.61186 ]\n",
      " [1.42751 ]\n",
      " [1.85334 ]\n",
      " [1.13273 ]\n",
      " [2.30356 ]\n",
      " [1.62814 ]\n",
      " [2.26892 ]\n",
      " [2.05954 ]\n",
      " [1.87817 ]\n",
      " [1.81657 ]\n",
      " [1.60839 ]\n",
      " [2.27759 ]\n",
      " [2.19119 ]\n",
      " [1.3815  ]\n",
      " [1.97161 ]\n",
      " [1.33961 ]\n",
      " [1.65645 ]\n",
      " [1.7482  ]\n",
      " [1.25702 ]\n",
      " [2.08295 ]\n",
      " [1.33036 ]\n",
      " [2.31798 ]\n",
      " [2.24716 ]\n",
      " [1.88906 ]\n",
      " [2.05117 ]\n",
      " [1.71805 ]\n",
      " [1.8554  ]\n",
      " [2.51809 ]\n",
      " [1.61483 ]\n",
      " [1.99541 ]\n",
      " [2.23522 ]\n",
      " [1.3147  ]\n",
      " [2.26721 ]\n",
      " [2.53897 ]\n",
      " [2.08785 ]\n",
      " [2.48701 ]\n",
      " [1.81742 ]\n",
      " [2.12599 ]\n",
      " [1.83339 ]\n",
      " [0.792423]\n",
      " [1.69301 ]\n",
      " [2.31182 ]\n",
      " [1.81657 ]\n",
      " [1.47715 ]\n",
      " [1.33036 ]\n",
      " [2.54628 ]\n",
      " [1.79833 ]\n",
      " [1.79241 ]\n",
      " [2.29285 ]\n",
      " [1.73494 ]\n",
      " [1.05876 ]\n",
      " [2.11187 ]\n",
      " [2.46945 ]\n",
      " [1.27831 ]\n",
      " [1.69036 ]\n",
      " [1.69589 ]\n",
      " [1.55991 ]\n",
      " [2.03104 ]\n",
      " [1.45489 ]\n",
      " [2.34442 ]\n",
      " [2.04389 ]\n",
      " [1.57934 ]\n",
      " [1.73739 ]\n",
      " [1.96025 ]\n",
      " [2.2121  ]\n",
      " [1.14984 ]\n",
      " [2.25678 ]\n",
      " [2.08336 ]\n",
      " [1.77853 ]\n",
      " [1.79516 ]\n",
      " [2.11329 ]\n",
      " [2.02782 ]\n",
      " [2.17889 ]\n",
      " [1.67459 ]\n",
      " [2.16785 ]\n",
      " [1.17776 ]\n",
      " [2.15827 ]\n",
      " [1.6798  ]\n",
      " [1.57238 ]\n",
      " [1.91877 ]\n",
      " [1.33336 ]\n",
      " [1.40161 ]\n",
      " [1.77345 ]\n",
      " [1.69494 ]\n",
      " [1.84966 ]\n",
      " [0.750477]\n",
      " [1.66566 ]\n",
      " [1.93642 ]\n",
      " [1.84295 ]\n",
      " [2.26892 ]\n",
      " [1.29474 ]\n",
      " [1.65645 ]\n",
      " [1.63128 ]\n",
      " [1.83489 ]\n",
      " [2.23522 ]\n",
      " [1.77171 ]\n",
      " [2.02609 ]\n",
      " [1.88911 ]\n",
      " [2.19119 ]\n",
      " [1.88906 ]\n",
      " [1.82798 ]\n",
      " [1.58039 ]\n",
      " [1.77714 ]\n",
      " [2.07405 ]\n",
      " [1.59921 ]\n",
      " [1.93642 ]\n",
      " [1.97205 ]\n",
      " [1.54152 ]\n",
      " [1.21163 ]\n",
      " [1.85344 ]\n",
      " [1.85067 ]\n",
      " [1.51859 ]\n",
      " [2.1587  ]\n",
      " [2.28342 ]\n",
      " [1.29474 ]\n",
      " [1.82499 ]\n",
      " [1.87214 ]\n",
      " [2.26494 ]\n",
      " [1.73636 ]\n",
      " [1.81956 ]\n",
      " [2.0218  ]\n",
      " [1.93609 ]\n",
      " [1.80795 ]\n",
      " [1.84636 ]\n",
      " [1.76639 ]\n",
      " [2.32157 ]\n",
      " [1.85067 ]\n",
      " [2.02782 ]\n",
      " [2.25678 ]\n",
      " [2.26721 ]\n",
      " [2.22554 ]\n",
      " [1.94453 ]\n",
      " [2.04113 ]\n",
      " [2.26413 ]\n",
      " [1.81104 ]\n",
      " [1.95798 ]\n",
      " [2.48717 ]\n",
      " [1.10366 ]\n",
      " [1.20302 ]\n",
      " [2.24244 ]\n",
      " [1.463   ]\n",
      " [2.03938 ]\n",
      " [1.17559 ]\n",
      " [2.22604 ]\n",
      " [1.65373 ]\n",
      " [1.84966 ]\n",
      " [1.93594 ]\n",
      " [2.34686 ]]\n"
     ]
    }
   ],
   "source": [
    "print(training_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
